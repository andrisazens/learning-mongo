1. =============================================== /// ===================================================

Problems 1 through 3 are an exercise in running mongod's, replica sets, and an exercise in testing of replica set rollbacks, which can occur when a former primary rejoins a set after it has previously had a failure.
Get the files from Download Handout link, and extract them. Use a.bat instead of a.sh on Windows.
Start a 3 member replica set (with default options for each member, all are peers). (a.sh will start the mongod's for you if you like.)

# if on unix:
chmod +x a.sh
./a.sh
You will need to initiate the replica set next.

Run:

mongo --shell --port 27003 a.js
// ourinit() will initiate the set for us.
// to see its source code type without the parentheses:
ourinit

// now execute it:
ourinit()
We will now do a test of replica set rollbacks. This is the case where data never reaches a majority of the set. We'll test a couple scenarios.

Now, let's do some inserts. Run:

db.foo.insert( { _id : 1 }, { writeConcern : { w : 2 } } )
db.foo.insert( { _id : 2 }, { writeConcern : { w : 2 } } )
db.foo.insert( { _id : 3 }, { writeConcern : { w : 2 } } )
Note: if 27003 is not primary, make it primary -- using rs.stepDown() on the mongod on port 27001 (perhaps also rs.freeze()) for example.

Next, let's shut down that server on port 27001:

var a = connect("localhost:27001/admin");
a.shutdownServer()
rs.status()
At this point the mongod on 27001 is shut down. We now have only our 27003 mongod, and our arbiter on 27002, running.

Let's insert some more documents:

db.foo.insert( { _id : 4 } )
db.foo.insert( { _id : 5 } )
db.foo.insert( { _id : 6 } )
Now, let's restart the mongod that is shut down. If you like you can cut and paste the relevant mongod invocation from a.sh.

Now run ps again and verify three are up:

ps -A | grep mongod
Now, we want to see if any data that we attempted to insert isn't there. Go into the shell to any member of the set. Use rs.status() to check state. Be sure the member is "caught up" to the latest optime (if it's a secondary). Also on a secondary you might need to invoke rs.slaveOk() before doing a query.)

Now run:

db.foo.find()
to see what data is there after the set recovered from the outage. How many documents do you have?

=> 6

2. =============================================== /// ===================================================

Let's do that again with a slightly different crash/recover scenario for each process. Start with the following:

With all three members (mongod's) up and running, you should be fine; otherwise, delete your data directory, and, once again:

./a.sh
mongo --shell --port 27003 a.js
ourinit() // you might need to wait a bit after this.
// be sure 27003 is the primary.
// use rs.stepDown() elsewhere if it isn't.
db.foo.drop()
db.foo.insert( { _id : 1 }, { writeConcern : { w : 2 } } )
db.foo.insert( { _id : 2 }, { writeConcern : { w : 2 } } )
db.foo.insert( { _id : 3 }, { writeConcern : { w : 2 } } )
var a = connect("localhost:27001/admin");
a.shutdownServer()
rs.status()
db.foo.insert( { _id : 4 } )
db.foo.insert( { _id : 5 } )
db.foo.insert( { _id : 6 } )
Now this time, shut down the mongod on port 27003 (in addition to the other member being shut down already) before doing anything else. One way of doing this in Unix would be:

ps -A | grep mongod
# should see the 27003 and 27002 ones running (only)
ps ax | grep mongo | grep 27003 | awk '{print $1}' | xargs kill
# wait a little for the shutdown perhaps...then:
ps -A | grep mongod
# should get that just the arbiter is presentâ¦
Now restart just the 27001 member. Wait for it to get healthy -- check this with rs.status() in the shell. Then query
// on windows
netstat -a -n -o // all processss with ports
taskkill -pid <port>

> db.foo.find()
Then add another document:

> db.foo.insert( { _id : "last" } )
After this, restart the third set member (mongod on port 27003). Wait for it to come online and enter a health state (secondary or primary).

Run (on any member -- try multiple if you like) :

> db.foo.find()
You should see a difference from problem 1 in the result above.

Question: Which of the following are true about mongodb's operation in these scenarios? Check all that apply.

MongoDB preserves the order of writes in a collection in its consistency model. In this problem, 27003's oplog was effectively a "fork" and to preserve write ordering a rollback was necessary during 27003's recovery phase.
The MongoDB primary does not write to its data files until a majority acknowledgement comes back from the rest of the cluster. When 27003 was primary, it did not perform the last 3 writes.
When 27003 came back up, it transmitted its write ops that the other member had not yet seen so that it would also have them.

3. =============================================== /// ===================================================

In question 2 the mongod on port 27003 does a rollback. Go to that mongod's data directory. Look for a rollback directory inside. Find the .bson file there. Run the bsondump utility on that file. What are its contents?

There is no such file.
It contains 2 documents.
It contains 3 documents. OK
It contains 4 documents.
It contains 8 documents.
The file exists but is 0 bytes long.

>bsondump data/z3/rollback/test.foo.2016-07-10T04-51-05.0.bson
{"_id":4.0}
{"_id":5.0}
{"_id":6.0}
2016-07-10T08:04:40.829+0300    3 objects found

4. =============================================== /// ===================================================

cfg = rs.conf()
cfg.members[2].priority = 0
rs.reconfig(cfg)

part4() => 233

5. =============================================== /// ===================================================

Suppose we have blog posts in a (not sharded*) postings collection, of the form:

{
  _id : ...,
  author : 'joe',
  title : 'Too big to fail',
  text : '...',
  tags : [ 'business', 'finance' ],
  when : ISODate("2008-11-03"),
  views : 23002,
  votes : 4,
  voters : ['joe', 'jane', 'bob', 'somesh'],
  comments : [
    { commenter : 'allan',
      comment : 'Well, i don't think soâ¦',
      flagged : false,
      plus : 2
    },
    ...
  ]
}
Which of these statements is true?

Note: to get a multiple answer question right in this final you must get all the components right, so even if some parts are simple, take your time.

*Certain restrictions apply to unique constraints on indexes when sharded, so I mention this to be clear.

a. We can create an index to make the following query fast/faster:

db.postings.find( { "comments.flagged" : true } )

b. One way to assure people vote at most once per posting is to use this form of update:

db.postings.update(
  { _id: . . . },
  { $inc : {votes:1}, $push : {voters:'joe'} }
);
combined with an index on { voters : 1 } which has a unique key constraint.


c.One way to assure people vote at most once per posting is to use this form of update:

db.postings.update(
  { _id: . . . , voters:{$ne:'joe'} },
  { $inc : {votes:1}, $push : {voters:'joe'} } );

A => a,b

6. =============================================== /// ===================================================

Which of these statements is true?

Note: to get a multiple answer question right in this final you must get all the components right, so even if some parts are simple, take your time.


a. MongoDB (v3.0) supports transactions spanning multiple documents, if those documents all reside on the same shard.
b. MongoDB has a data type for binary data.
c. MongoDB supports atomic operations on individual documents.
d. MongoDB allows you to choose the storage engine separately for each collection on your mongod.

b,c,d

7. =============================================== /// ===================================================

Which of these statements is true?

a. MongoDB is "multi-master" -- you can write anywhere, anytime.
b. MongoDB supports reads from slaves/secondaries that are in remote locations.
c. Most MongoDB queries involve javascript execution on the database server(s).

b

8. =============================================== /// ===================================================

mongorestore gene_backup\s1 --port 27018
mongorestore gene_backup\s2 --port 27020
mongorestore gene_backup\config_server --port 27019

start mongod --shardsvr --dbpath C:\Data\mongo_exam\s1 --logpath C:\Data\mongo_exam\log_s1.txt --logappend --port 27018 --smallfiles --oplogSize 50
start mongod --shardsvr --dbpath C:\Data\mongo_exam\s2 --logpath C:\Data\mongo_exam\log_s2.txt --logappend --port 27020 --smallfiles --oplogSize 50
start mongod --configsvr --dbpath C:\Data\mongo_exam\config --logpath C:\Data\mongo_exam\log_config.txt --logappend --port 27019 --smallfiles --oplogSize 50

mongorestore gene_backup\config_server --port 27019
mongorestore gene_backup\s1 --port 27018

We have been asked by our users to pull some data from a previous database backup of a sharded cluster. They'd like us to set up a temporary data mart for this purpose, in addition to answering some questions from the data. The next few questions involve this user request.

First we will restore the backup. Download gene_backup.zip from the Download Handout link and unzip this to a temp location on your computer.

The original cluster that was backed up consisted of two shards, each of which was a three member replica set. The first one named "s1" and the second "s2". We have one mongodump (backup) for each shard, plus the config database. After you unzip you will see something like this:

$ ls -la
total 0
drwxr-xr-x   5 dwight  staff  170 Dec 11 13:47 .
drwxr-xr-x  17 dwight  staff  578 Dec 11 13:49 ..
drwxr-xr-x   4 dwight  staff  136 Dec 11 13:45 config_server
drwxr-xr-x   5 dwight  staff  170 Dec 11 13:46 s1
drwxr-xr-x   5 dwight  staff  170 Dec 11 13:46 s2
Our data mart will be temporary, so we won't need more than one mongod per shard, nor more than one config server (we are not worried about downtime, the mart is temporary).

As a first step, restore the config server backup and run a mongod config server instance with that restored data. The backups were made with mongodump. Thus you will use the mongorestore utility to restore.

Once you have the config server running, confirm the restore of the config server data by running the last javascript line below in the mongo shell, and entering the 5 character result it returns.

$ mongo localhost:27019/config
configsvr>
configsvr> db
config
configsvr> db.chunks.find().sort({_id:1}).next().lastmodEpoch.getTimestamp().toUTCString().substr(20,6) => 39:15

9. =============================================== /// ===================================================

